{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lStpei-kK9Ev"
      },
      "source": [
        "# CAMP II Surgical Workflow Analysis Exercise\n",
        "\n",
        "It is highly recommended to complete the CAMP II Exercises on Classification and Segmentation before this one!  \n",
        "Feel free to reuse code you wrote for these other exercises here, it might be useful. Make sure to activate the GPU in colab before you start this exercise.\n",
        "\n",
        "In this exercise, we will be making first steps towards workflow analysis from endoscopic data. Specifically, we will be extracting the surgical phases, such as \"Preparation\" or \"ColotTriangleDissection\". \n",
        "\n",
        "We will be working with an extended version of the dataset used in the exercise classification and segmentation. The extension allows us to observe the full workflow steps in every video.\n",
        "\n",
        "This notebook has many codeblocks already in place to help you get started. Places where you have to add your own code are clearly marked with \"TASK\" and lines (\"-----\"). When a variable you have to implement is used later on, we placed a name and description in the task bracket (see example below). These markings are only there to guide you toward what you have to implement to complete the exercise, feel free to experiment beyond them.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xiI2LeWncRs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: description of the task you need to do ---------------------------------\n",
        "# my_variable_name: a variable that is used later on, so the name should be right\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhcBSq8_ndNI"
      },
      "source": [
        "When looking for solutions to the tasks below, definitely consider online resources such as the ones linked below:\n",
        "\n",
        "Pytorch (torch) documentation: <br>\n",
        "https://pytorch.org/docs/stable/index.html\n",
        "\n",
        "Pytorch Vision (torchvision) documentation: <br>\n",
        "https://pytorch.org/vision/stable/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpfxHfdbLM_Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1MwtcHceqj8FchmsIR92pRb0tR-QZ9uh4 \n",
            "\n",
            "unzip:  cannot find or open liver_endoscopy_dataset_workflow.zip, liver_endoscopy_dataset_workflow.zip.zip or liver_endoscopy_dataset_workflow.zip.ZIP.\n",
            "rm: cannot remove ‘liver_endoscopy_dataset_workflow.zip’: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# download the dataset to your notebook\n",
        "# if you get access denied, retry after a minute\n",
        "!gdown 1MwtcHceqj8FchmsIR92pRb0tR-QZ9uh4\n",
        "!unzip -qq liver_endoscopy_dataset_workflow.zip \n",
        "!rm liver_endoscopy_dataset_workflow.zip\n",
        "# this block should take 1 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc-EIaiB_ofh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip -qq install pytorch_lightning==1.6.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1dtsCPSYvM"
      },
      "source": [
        "We import the libraries that will be useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUZPkdE2K9Ey",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import BoundaryNorm, ListedColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD7MIt9sY3VD"
      },
      "source": [
        "## 0. Dataset\n",
        "We load the liver endoscopy dataset here. This cell defines the video splits, correctly loads the dataset depending on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuE42kWXYvqe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "video_splits = {'train': ['01', '02', '05', '13', '15', '18', '22'], 'val': ['08', '29', '50'], 'test': ['06', '10', '42']}\n",
        "\n",
        "\n",
        "class LiverEndoscopy(Dataset):\n",
        "    def __init__(self, split: str = 'train', balance_data: bool = False, temporal: bool = False,\n",
        "                 pil_transform: Optional[transforms.Compose] = None, tensor_transform: Optional[transforms.Compose] = None, augmentation=None):\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.split = split\n",
        "        self.balance_data = balance_data\n",
        "        self.temporal = temporal\n",
        "        self.pil_transform = pil_transform\n",
        "        self.tensor_transform = tensor_transform\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "        self.phases_to_indices = {'Preparation': 0, 'CalotTriangleDissection': 1, 'ClippingCutting': 2, 'GallbladderDissection': 3, 'GallbladderPackaging': 4,\n",
        "                                  'CleaningCoagulation': 5, 'GallbladderRetraction': 6}\n",
        "        self.indices_to_phases = {value: key for key, value in self.phases_to_indices.items()}\n",
        "\n",
        "        export_dataset_path = Path('data_workflow')\n",
        "        self.images_path = export_dataset_path / 'images'\n",
        "        with open(export_dataset_path / 'phase_annotations.json', 'r') as f:\n",
        "            self.workflow_phase_annotations = json.load(f)\n",
        "\n",
        "        if not temporal:\n",
        "            self.image_names = []\n",
        "            for image_path in sorted(self.images_path.glob('*.png')):\n",
        "                video_id = image_path.name.split('_')[0].replace('video', '')\n",
        "                if video_id in video_splits[split]:\n",
        "                    self.image_names.append(image_path.name.replace('.png', ''))\n",
        "            self.image_names = sorted(self.image_names)\n",
        "\n",
        "        else:\n",
        "            self.window_size = 8\n",
        "            self.downsample_factor = 25\n",
        "            all_image_names = sorted(self.images_path.glob('*.png'))\n",
        "            all_video_ids = {image_name.name.split('_')[0].replace('video', '') for image_name in all_image_names}\n",
        "            all_split_video_ids = {video_id for video_id in all_video_ids if video_id in video_splits[split]}\n",
        "            self.windows = []\n",
        "            for video_id in all_split_video_ids:\n",
        "                sequence_images = sorted(self.images_path.glob(f'video{video_id}_*.png'))\n",
        "                sequence_image_indices = [int(image_name.name.split('_')[1].replace('.png', '')) for image_name in sequence_images]\n",
        "                for i in range(len(sequence_images) - self.window_size + 1):\n",
        "                    self.windows.append((video_id, sequence_image_indices[i:i + self.window_size]))\n",
        "\n",
        "        if balance_data:\n",
        "            self.do_balance_data(temporal)\n",
        "\n",
        "    def do_balance_data(self, temporal):\n",
        "        print('Balancing data by oversampling under-represented classes...')\n",
        "        class_to_samples = defaultdict(list)\n",
        "        if not temporal:\n",
        "            for image_name in self.image_names:\n",
        "                label = self.workflow_phase_annotations[image_name]\n",
        "                class_to_samples[label].append(image_name)\n",
        "            max_number = max([len(elem) for elem in class_to_samples.values()])\n",
        "            self.image_names = []\n",
        "            for key, value in class_to_samples.items():\n",
        "                if len(value) < max_number:\n",
        "                    self.image_names += random.choices(value, k=max_number)\n",
        "                else:\n",
        "                    self.image_names += value\n",
        "            random.shuffle(self.image_names)\n",
        "        else:\n",
        "            for video_id, window in self.windows:\n",
        "                label = self.workflow_phase_annotations[f'video{video_id}_{str(window[-1]).zfill(6)}']\n",
        "                class_to_samples[label].append((video_id, window))\n",
        "            max_number = max([len(elem) for elem in class_to_samples.values()])\n",
        "            self.windows = []\n",
        "            for key, value in class_to_samples.items():\n",
        "                if len(value) < max_number:\n",
        "                    self.windows += random.choices(value, k=max_number)\n",
        "                else:\n",
        "                    self.windows += value\n",
        "            random.shuffle(self.windows)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.temporal:\n",
        "            return len(self.windows)\n",
        "        else:\n",
        "            return len(self.image_names)\n",
        "\n",
        "    def phase_label_to_number(self, phase_label):\n",
        "        return self.phases_to_indices[phase_label]\n",
        "\n",
        "    def number_to_phase_label(self, phase_number):\n",
        "        return self.indices_to_phases[phase_number]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.temporal:\n",
        "            video_id, window = self.windows[index]\n",
        "            image_names = []\n",
        "            for frame_number in window:\n",
        "                image_names.append(f'video{video_id}_{str(frame_number).zfill(6)}.png')\n",
        "\n",
        "            phase = self.phase_label_to_number(self.workflow_phase_annotations[image_names[-1].replace('.png', '')])\n",
        "            return {'image_names': image_names, 'phase': phase}\n",
        "        else:\n",
        "            image_name = self.image_names[index]\n",
        "            image_path = self.images_path / f'{image_name}.png'\n",
        "            image = Image.open(image_path)\n",
        "            if self.pil_transform is not None:\n",
        "                image = self.pil_transform(image)\n",
        "            if self.augmentation is not None:\n",
        "                image = self.augmentation(image)\n",
        "            image_tensor = transforms.ToTensor()(image)\n",
        "            if self.tensor_transform is not None:\n",
        "                image_tensor = self.tensor_transform(image_tensor)\n",
        "\n",
        "            phase = self.phase_label_to_number(self.workflow_phase_annotations[image_name])\n",
        "\n",
        "            return {'image': image_tensor, 'phase': phase, 'image_name': image_name}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JtaPQcHK9E1"
      },
      "source": [
        "## A. Workflow Recognition without Temporal Modelling\n",
        "\n",
        "First, we will try to recognize the different phases of the surgical workflow from each frame of the video. This is equivalent to a multi-class classification problem on each frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMLQv8C9K9E2"
      },
      "source": [
        "### A.1 Load the data as images with phase labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik5O-agaK9E2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Fix pytorch_lightning seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# load data\n",
        "pil_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
        "augmentation = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.ColorJitter(brightness=.1, hue=.1),\n",
        "                                    transforms.RandomRotation(degrees=(0, 30)),\n",
        "                                    transforms.RandomResizedCrop((224, 224), scale=(0.7, 1.0))])\n",
        "\n",
        "train_dataset = LiverEndoscopy(split='train', balance_data=False, temporal=False, pil_transform=None, augmentation=augmentation)\n",
        "val_dataset = LiverEndoscopy(split='val', balance_data=False, temporal=False, pil_transform=pil_transform)\n",
        "test_dataset = LiverEndoscopy(split='test', balance_data=False, temporal=False, pil_transform=pil_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDr7OJplTsRj"
      },
      "source": [
        "#### Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSW_JQV37s1R",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: Load a random sample from the dataset, visualize the image and its phase label\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yDvucLK9E3"
      },
      "source": [
        "### A.2 Load and train a workflow recognition model\n",
        "We already provide most of the code skeleton in this part. To finish this model, you will need to make the following modifications.\n",
        "1. An image processing model (resnet18, pretrained=True), which you can find in torchvision.\n",
        "2. Disable the final layer (fc) of the image model\n",
        "3. Define a new linear layer, that takes as input the output of the image model, and outputs 7 nodes. These will correspond to the 7 classes.\n",
        "4. Define the forward function, which uses the image model and the linear layer\n",
        "5. Complete the training, validation and test steps, where the models forward function is called to get a prediction, and then a cross entropy loss between the correct phase and the prediction is computed. Tip: F.cross_entropy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okdpj4LmK9E4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ModelWrapper(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TASK: define image model, disable final layer, add linear layer ------\n",
        "\n",
        "\n",
        "        # ----------------------------------------------------------------------       \n",
        "\n",
        "        self.train_preds = []\n",
        "        self.train_gts = []\n",
        "        self.val_preds = []\n",
        "        self.val_gts = []\n",
        "        self.test_preds = []\n",
        "        self.test_gts = []\n",
        "        self.reset_metrics()\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.test_loss = []\n",
        "\n",
        "        self.phase_names = ['Preparation', 'ColotTriangleDissection', 'ClippingCutting', 'GallbladderDissection', 'GallbladderPackaging', 'CleaningCoagulation',\n",
        "                            'GallbladderRetraction']\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TASK use the image model and linear layer to get a prediction ---------\n",
        "\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------       \n",
        "        self.update_metrics(batch['phase'], y_hat, split='train')\n",
        "        self.train_loss.append(loss.item())\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # validation_step defines the validation loop.\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.update_metrics(batch['phase'], y_hat, split='val')\n",
        "        self.val_loss.append(loss.item())\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # test_step defines the test loop.\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.update_metrics(batch['phase'], y_hat, split='test')\n",
        "        self.test_loss.append(loss.item())\n",
        "        return {'test_loss': loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "    def reset_metrics(self, split=None):\n",
        "        if split == 'train':\n",
        "            self.train_preds = []\n",
        "            self.train_gts = []\n",
        "        elif split == 'val':\n",
        "            self.val_preds = []\n",
        "            self.val_gts = []\n",
        "        elif split == 'test':\n",
        "            self.test_preds = []\n",
        "            self.test_gts = []\n",
        "        else:\n",
        "            self.train_preds = []\n",
        "            self.train_gts = []\n",
        "            self.val_preds = []\n",
        "            self.val_gts = []\n",
        "            self.test_preds = []\n",
        "            self.test_gts = []\n",
        "\n",
        "    def update_metrics(self, gt, pred, split='train'):\n",
        "        if split == 'train':\n",
        "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
        "        elif split == 'val':\n",
        "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
        "        elif split == 'test':\n",
        "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='train')\n",
        "        self.reset_metrics(split='train')\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='val')\n",
        "        self.reset_metrics(split='val')\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='test')\n",
        "        self.reset_metrics(split='test')\n",
        "\n",
        "    def evaluate_predictions(self, split):\n",
        "        if split == 'train':\n",
        "            preds = self.train_preds\n",
        "            gts = self.train_gts\n",
        "        elif split == 'val':\n",
        "            preds = self.val_preds\n",
        "            gts = self.val_gts\n",
        "        elif split == 'test':\n",
        "            preds = self.test_preds\n",
        "            gts = self.test_gts\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        cls_report = classification_report(gts, preds, labels=list(range(len(self.phase_names))),\n",
        "                                           target_names=self.phase_names)\n",
        "        print(split)\n",
        "        print(cls_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4Ow32GVMDv"
      },
      "source": [
        "#### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fPcebr62B6f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: create the model -------------------------------------------------------\n",
        "# model: your model\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZHA73roTyzE"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef26-VL_CCNy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: train the model --------------------------------------------------------\n",
        "# TIP: use pl.Trainer, make sure to use the gpu and train for 4 epochs, -> max_epochs=4. Make sure to call the fit function with both the train and validation loaders to get correct evaluations during training\n",
        "# TIP: Training should take around 10 mins\n",
        "\n",
        "# trainer: your trainer\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBhY8oa470t"
      },
      "source": [
        "#### Save the model!\n",
        "\n",
        "Training a neural network takes some time. If you don't want to loose this progress, e.g. because you need to take a break from this exercise, make sure to save the model with the code below. Then download the created file '\\<myModel\\>.pt'!\n",
        "\n",
        "If you want to continue with your model later, you can use the code provided below to load your model after uploading it to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQBCUxcs4uvO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'model_state_dict.pt')\n",
        "\n",
        "# load model (instead of training if you have a saved model)\n",
        "# model.load_state_dict(torch.load('model_state_dict.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmh2rLp4K9E7"
      },
      "source": [
        "### A.3 Evaluate the results\n",
        "#### Test the model on the unseen test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7VuQl9POpFw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: Test the model on the unseen test set ----------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeuiSYCmWKjj"
      },
      "source": [
        "#### Plot the loss to see the training progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsiNukkkK9E8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: plot loss --------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asrrcx1aWQQs"
      },
      "source": [
        "#### Visualize Predictions\n",
        "\n",
        "Similar to before, get a random sample from the test_dataset, run it through the model in evaluation mode to get a prediction, then visualize the image, the prediction and the ground truth. Get an example for every phase from the ground truth and look at the image and network prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRKH0UV-V9wI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: visualize predition results --------------------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQE_u-hFqNQc"
      },
      "source": [
        "#### Visualize the predictions for a whole video\n",
        "\n",
        "To see a complete predicted workflow we want to plot the predictions for a whole video as a sequence. To achieve that we first have to get the predictions for a whole video in sequence from the model and then plot them with the ground truth for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARHU0KUhj5gt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "# TASK: set up the model for evaluation ----------------------------------------\n",
        "# TIP: freeze the model, set to eval mode, transfer to gpu\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "video_name = \"video06\" # \"video10\" , \"video42\"\n",
        "video_preds = []\n",
        "video_labels = []\n",
        "for batch in tqdm(test_loader):\n",
        "  if batch['image_name'][0].startswith(video_name):\n",
        "    with torch.no_grad():\n",
        "      # TASK: transfer the input to the gpu and evaluate with your model -------\n",
        "      # batch_preds: Predictions of you model for the current batch\n",
        "\n",
        "\n",
        "      # ------------------------------------------------------------------------\n",
        "      for image_name, pred, label in zip(batch['image_name'], batch_preds, batch['phase']):\n",
        "          if image_name.startswith(video_name):\n",
        "            video_preds.append(pred.detach().cpu().numpy().argmax())\n",
        "            video_labels.append(label.detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyU8FBTtj6ug",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the predicted workflow\n",
        "fig = plt.figure()\n",
        "cmap= ListedColormap(sns.color_palette(\"muted\", as_cmap=True))\n",
        "levels= list(range(len(model.phase_names) + 1))\n",
        "norm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
        "\n",
        "ax1 = fig.add_axes([0, 0.4, 2, 0.3])\n",
        "barprops1 = dict(aspect='auto', cmap=cmap, norm=norm, interpolation='nearest')\n",
        "im1 = ax1.imshow(np.array(video_preds).reshape(1,-1), **barprops1)\n",
        "ax1.set_axis_off()\n",
        "ax1.set_title('Prediction')\n",
        "\n",
        "ax2 = fig.add_axes([0, 0, 2, 0.3])\n",
        "barprops2 = dict(aspect='auto', cmap=cmap, norm=norm, interpolation='nearest')\n",
        "im2 = ax2.imshow(np.array(video_labels).reshape(1,-1), **barprops2)\n",
        "ax2.set_axis_off()\n",
        "ax2.set_title('Ground Truth')\n",
        "\n",
        "cbar_ax = fig.add_axes([2.1, 0, 0.05, 0.7])\n",
        "cbar = fig.colorbar(im1, cax=cbar_ax)\n",
        "cbar.set_ticks([x + 0.5 for x in range(7)])\n",
        "cbar.set_ticklabels(model.phase_names)\n",
        "cbar.ax.invert_yaxis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_b5OMi3K9E9"
      },
      "source": [
        "## B. Workflow Recognition with Temporal Modelling\n",
        "\n",
        "Recognizing the current phase of the surgery from a single image can be very difficult, even for a human expert. Including the temporal context in your phase prediction can be very helpful, so we will try to create a machine learning model that can do just that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IblvQk1JK9E-"
      },
      "source": [
        "### B.1 Load the data as video clips with a phase label\n",
        "\n",
        "For this exercise we want to include the temporal context in each prediction. We accomplish this by looking at the surgical videos contained in the dataset not as individual frames, but as short video clips.\n",
        "Load the data so that you have sequences of 8 <b>consecutive</b> frames and the phase label of the last frame of each such sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HEVc6bwK9E_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "train_dataset_temp = LiverEndoscopy(split='train', balance_data=False, temporal=True)\n",
        "val_dataset_temp = LiverEndoscopy(split='val', balance_data=False, temporal=True)\n",
        "test_dataset_temp = LiverEndoscopy(split='test', balance_data=False, temporal=True)\n",
        "train_loader_temp = DataLoader(train_dataset_temp, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader_temp = DataLoader(val_dataset_temp, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader_temp = DataLoader(test_dataset_temp, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QM7tmwAK9FA"
      },
      "source": [
        "### B.2 Extract Image Features\n",
        "\n",
        "We will use the model you previously trained as a feature extractor. This means, we will run every image in all the datasets through the model, and get the image features (not the phase predictions).\n",
        "TASK: To this end, you first need to disable the linear layer of the model.\n",
        "Make sure to set it  to evaluate model, transfer it to gpu, and freeze it as well.\n",
        "\n",
        "Then we create a dictionary called all_features, where the keys will be the image names, and the values will be the corresponding image features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxBCTolaK9FB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: set model up to extract features ---------------------------------------\n",
        "# TIP: freeze the model, set to eval mode, transfer to gpu and disable the linear layer\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# recreate the train dataset without augmentions\n",
        "train_dataset = LiverEndoscopy(split='train', balance_data=False, temporal=False, pil_transform=pil_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# TASK: iterate through the data, extract features with your model, write to all_features\n",
        "# TIP: feature extraction should take 5 min to run\n",
        "# TIP: extract features for all sets (train, val, test)\n",
        "all_features = {} # Keys: image names, Features: corresponding image features\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht5Bhs-sJV8z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#Simple test for checking the validity of all features\n",
        "assert all_features['video01_000276'].shape[0] == 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XKjUKcK9FC"
      },
      "source": [
        "### B.3 Simple Temporal model\n",
        "\n",
        "Now you will design a temporal model, that will take the extracted image features as input and predict the surgical phase of the last frame in the sequence.\n",
        "\n",
        "Many types of models can be used here. We will go for a very simple option. We will concatenate the image features from 8 neigbouring images, and feed it through 2 linear layers to make a final prediction of 7 phases again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT_4Na5EK9FC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# build mlp\n",
        "class TemporalModelWrapper(pl.LightningModule):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        # TASK: Desfine you model layers ---------------------------------------\n",
        "        # TIP: First layer: Input size: 512x8 -> output 256. \n",
        "        # TIP: Second Layer: Input size: 256 -> output 7\n",
        "\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.features = features\n",
        "\n",
        "        self.train_preds = []\n",
        "        self.train_gts = []\n",
        "        self.val_preds = []\n",
        "        self.val_gts = []\n",
        "        self.test_preds = []\n",
        "        self.test_gts = []\n",
        "        self.reset_metrics()\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.test_loss = []\n",
        "\n",
        "        self.phase_names = ['Preparation', 'ColotTriangleDissection', 'ClippingCutting', 'GallbladderDissection', 'GallbladderPackaging', 'CleaningCoagulation',\n",
        "                            'GallbladderRetraction']\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TASK: define your network --------------------------------------------\n",
        "        # TIP: x has the shape Batch_size x 8 x 512 -> You need to reshape it to batch_size x 4096 and then pass it through the fully connected layers\n",
        "        \n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        # We get the pre computed image features\n",
        "        all_image_features = []\n",
        "        for image_names in batch['image_names']:\n",
        "            batch_image_features = [self.features[image_name.replace('.png', '')] for image_name in image_names]\n",
        "            all_image_features.append(torch.stack(batch_image_features))\n",
        "        all_image_features = torch.stack(all_image_features).transpose(0, 1)\n",
        "        all_image_features = all_image_features.to(self.device)\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.update_metrics(batch['phase'], y_hat, split='train')\n",
        "        self.train_loss.append(loss.item())\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # validation_step defines the validation loop.\n",
        "        all_image_features = []\n",
        "        for image_names in batch['image_names']:\n",
        "            batch_image_features = [self.features[image_name.replace('.png', '')] for image_name in image_names]\n",
        "            all_image_features.append(torch.stack(batch_image_features))\n",
        "        all_image_features = torch.stack(all_image_features).transpose(0, 1)\n",
        "        all_image_features = all_image_features.to(self.device)\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.update_metrics(batch['phase'], y_hat, split='val')\n",
        "        self.val_loss.append(loss.item())\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # test_step defines the test loop.\n",
        "        all_image_features = []\n",
        "        for image_names in batch['image_names']:\n",
        "            batch_image_features = [self.features[image_name.replace('.png', '')] for image_name in image_names]\n",
        "            all_image_features.append(torch.stack(batch_image_features))\n",
        "        all_image_features = torch.stack(all_image_features).transpose(0, 1)\n",
        "        all_image_features = all_image_features.to(self.device)\n",
        "        # TASK: get a prediction from the model and calculate cross_entropy loss.\n",
        "        # y_hat: prediction from the model\n",
        "        # loss: calculated loss from the model\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        self.update_metrics(batch['phase'], y_hat, split='test')\n",
        "        self.test_loss.append(loss.item())\n",
        "        return {'test_loss': loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "    def reset_metrics(self, split=None):\n",
        "        if split == 'train':\n",
        "            self.train_preds = []\n",
        "            self.train_gts = []\n",
        "        elif split == 'val':\n",
        "            self.val_preds = []\n",
        "            self.val_gts = []\n",
        "        elif split == 'test':\n",
        "            self.test_preds = []\n",
        "            self.test_gts = []\n",
        "        else:\n",
        "            self.train_preds = []\n",
        "            self.train_gts = []\n",
        "            self.val_preds = []\n",
        "            self.val_gts = []\n",
        "            self.test_preds = []\n",
        "            self.test_gts = []\n",
        "\n",
        "    def update_metrics(self, gt, pred, split='train'):\n",
        "        if split == 'train':\n",
        "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
        "        elif split == 'val':\n",
        "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
        "        elif split == 'test':\n",
        "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
        "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='train')\n",
        "        self.reset_metrics(split='train')\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='val')\n",
        "        self.reset_metrics(split='val')\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        self.evaluate_predictions(split='test')\n",
        "        self.reset_metrics(split='test')\n",
        "\n",
        "    def evaluate_predictions(self, split):\n",
        "        if split == 'train':\n",
        "            preds = self.train_preds\n",
        "            gts = self.train_gts\n",
        "        elif split == 'val':\n",
        "            preds = self.val_preds\n",
        "            gts = self.val_gts\n",
        "        elif split == 'test':\n",
        "            preds = self.test_preds\n",
        "            gts = self.test_gts\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        cls_report = classification_report(gts, preds, labels=list(range(len(self.phase_names))),\n",
        "                                           target_names=self.phase_names)\n",
        "        print(split)\n",
        "        print(cls_report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX34y0K-ZV6d"
      },
      "source": [
        "#### Create and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gXgfnCy1zqM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: create the temporal model and train it ---------------------------------\n",
        "# TIP: again use pl.Trainer. Train for max_epochs=10 this time.\n",
        "# temporal_model: your temporal model\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6-5z3CEZYiS"
      },
      "source": [
        "### B.4 Evaluate the results\n",
        "\n",
        "#### Test the model\n",
        "\n",
        "If everything worked correctly, you should observe an improved performance when using temporal modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RVTNtGD9JVh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: Test the model on the unseen test set ----------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKbJF61LZbg3"
      },
      "source": [
        "#### Plot the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orguWYFh9KNr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# TASK: plot loss --------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3LWnBnKr3bN"
      },
      "source": [
        "#### Visualize the predictions for a whole video\n",
        "\n",
        "To see a complete predicted workflow we want to plot the predictions for a whole video as a sequence like we did above. We can also plot the results from our normal model to compare against the temporal model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ygXyQQjmCBL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "# TASK: set up the temporal model for evaluation -------------------------------\n",
        "# TIP: freeze the model, set to eval mode, transfer to gpu\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "video_name = \"video06\" # \"video10\" , \"video42\"\n",
        "video_preds_temp = []\n",
        "video_labels = []\n",
        "for batch in tqdm(test_loader_temp):\n",
        "  if batch['image_names'][0][-1].startswith(video_name):\n",
        "    with torch.no_grad():\n",
        "      # Get the pre computed image features\n",
        "      batch_image_features = []\n",
        "      for image_names in batch['image_names']:\n",
        "          image_features = [temporal_model.features[image_name.replace('.png', '')] for image_name in image_names]\n",
        "          batch_image_features.append(torch.stack(image_features))\n",
        "      batch_image_features = torch.stack(batch_image_features).transpose(0, 1)\n",
        "      # TASK: transfer the input to the gpu and evaluate with your model -------\n",
        "      # batch_preds: Predictions of you model for the current batch\n",
        "\n",
        "\n",
        "      # ------------------------------------------------------------------------\n",
        "      for pred, label in zip(batch_preds, batch['phase']):\n",
        "        video_preds_temp.append(pred.detach().cpu().numpy().argmax())\n",
        "        video_labels.append(label.detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAPOZWyr2o-B",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "cmap= ListedColormap(sns.color_palette(\"muted\", as_cmap=True))\n",
        "levels= list(range(len(model.phase_names) + 1))\n",
        "norm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
        "barprops = dict(aspect='auto', cmap=cmap, norm=norm, interpolation='nearest')\n",
        "\n",
        "ax1 = fig.add_axes([0, 0.8, 2, 0.3])\n",
        "# predictions for the normal model are not computed again\n",
        "im1 = ax1.imshow(np.array(video_preds).reshape(1,-1), **barprops)\n",
        "ax1.set_axis_off()\n",
        "ax1.set_title('Prediction')\n",
        "\n",
        "ax2 = fig.add_axes([0, 0.4, 2, 0.3])\n",
        "im2 = ax2.imshow(np.array(video_preds_temp).reshape(1,-1), **barprops)\n",
        "ax2.set_axis_off()\n",
        "ax2.set_title('Prediction with Temporal Model')\n",
        "\n",
        "ax3 = fig.add_axes([0, 0, 2, 0.3])\n",
        "im3 = ax3.imshow(np.array(video_labels).reshape(1,-1), **barprops)\n",
        "ax3.set_axis_off()\n",
        "ax3.set_title('Ground Truth')\n",
        "\n",
        "cbar_ax = fig.add_axes([2.1, 0.25, 0.05, 0.7])\n",
        "cbar = fig.colorbar(im1, cax=cbar_ax)\n",
        "cbar.set_ticks([x + 0.5 for x in range(7)])\n",
        "cbar.set_ticklabels(model.phase_names)\n",
        "cbar.ax.invert_yaxis()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PD7MIt9sY3VD"
      ],
      "name": "workflow_exercise.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8939ef2a51d490bca7e91ec38d59beed54b6b64a52526ae9dee62e8212aad3c4"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
