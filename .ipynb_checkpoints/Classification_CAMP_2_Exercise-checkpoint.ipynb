{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFmk542WO3mj"
   },
   "source": [
    "#Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-25 13:03:23--  https://nextcloud.in.tum.de/index.php/s/9tp3yCSiLgjLWdj/download/liver_endoscopy_dataset.zip\n",
      "Resolving nextcloud.in.tum.de (nextcloud.in.tum.de)... 131.159.0.29, 2a09:80c0::29\n",
      "Connecting to nextcloud.in.tum.de (nextcloud.in.tum.de)|131.159.0.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2907480775 (2.7G) [application/zip]\n",
      "Saving to: ‘liver_endoscopy_dataset.zip.1’\n",
      "\n",
      "100%[====================================>] 2,907,480,775 60.1MB/s   in 19s    \n",
      "\n",
      "2022-05-25 13:03:43 (149 MB/s) - ‘liver_endoscopy_dataset.zip.1’ saved [2907480775/2907480775]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nextcloud.in.tum.de/index.php/s/9tp3yCSiLgjLWdj/download/liver_endoscopy_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "75Rx11W3O2eX",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/icb/mostafa.shahhosseini/miniconda3/envs/mossi/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Access denied with the following error:\n",
      "\n",
      " \tToo many users have viewed or downloaded this file recently. Please\n",
      "\ttry accessing the file again later. If the file you are trying to\n",
      "\taccess is particularly large or is shared with many people, it may\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\n",
      "\tstill can't access a file after 24 hours, contact your domain\n",
      "\tadministrator. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=1MHp64mCt2m8NxCW3-4kjD39m3Rry0ekA \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1MHp64mCt2m8NxCW3-4kjD39m3Rry0ekA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsWp20qrPBYU"
   },
   "source": [
    "#Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDmyJRU0OsLz",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "  !unzip -qq liver_endoscopy_dataset\n",
    "  !rm liver_endoscopy_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUXIOdXkdGru"
   },
   "source": [
    "#Install Additional Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Kpwbxz-JdUUl",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -qq install pytorch_lightning==1.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ontRrJ6ycUyK"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bdM9w77RcZoB",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiLQ8AlZcbKi"
   },
   "source": [
    "#Dataset\n",
    "We load the liver endoscopy dataset here. This cell defines the video splits, correctly loads the dataset depending on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_e7ySAyZcarE",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "video_splits = {'train': ['01', '09', '17', '20', '25', '27', '28', '35', '37'], 'val': ['12','18', '24'], 'test': ['26', '48', '52', '43', '55']}\n",
    "\n",
    "class LiverEndoscopy(Dataset):\n",
    "    def __init__(self,\n",
    "                 task_type: str = 'classification', split: str = 'train', balance_data: bool = False, temporal: bool = False,\n",
    "                 pil_transform: Optional[transforms.Compose] = None, tensor_transform: Optional[transforms.Compose] = None):\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.split = split\n",
    "        self.balance_data = balance_data\n",
    "        self.task_type = task_type\n",
    "        self.temporal = temporal\n",
    "        self.pil_transform = pil_transform\n",
    "        self.tensor_transform = tensor_transform\n",
    "\n",
    "        export_dataset_path = Path('/home/icb/mostafa.shahhosseini/cls/data')\n",
    "        self.images_path = export_dataset_path / 'images'\n",
    "        self.seg_masks_path = export_dataset_path / 'seg_masks'\n",
    "        with open(export_dataset_path / 'classification_annotations.json', 'r') as f:\n",
    "            self.classification_annotations = json.load(f)\n",
    "        with open(export_dataset_path / 'phase_annotations.json', 'r') as f:\n",
    "            self.workflow_phase_annotations = json.load(f)\n",
    "        with open(export_dataset_path / 'has_liver.json', 'r') as f:\n",
    "            self.has_liver = json.load(f)\n",
    "\n",
    "        if task_type == 'classification' or task_type == 'segmentation' or (task_type == 'workflow' and not temporal):\n",
    "            self.image_names = []\n",
    "            for image_path in sorted(self.images_path.glob('*.png')):\n",
    "                video_id = image_path.name.split('_')[0].replace('video', '')\n",
    "                if video_id in video_splits[split]:\n",
    "                    self.image_names.append(image_path.name.replace('.png', ''))\n",
    "            self.image_names = sorted(self.image_names)\n",
    "\n",
    "        elif task_type == 'workflow' and temporal:\n",
    "            self.window_size = 8\n",
    "            self.downsample_factor = 5\n",
    "            chunks = []\n",
    "            start = -1\n",
    "            end = -1\n",
    "            prev_frame_number = None\n",
    "            prev_video_id = None\n",
    "            for image_path in sorted(self.images_path.glob('*.png')):\n",
    "                video_id = image_path.name.split('_')[0].replace('video', '')\n",
    "                if video_id not in video_splits[split]:\n",
    "                    continue\n",
    "                if prev_video_id is None:\n",
    "                    prev_video_id = video_id\n",
    "                elif prev_video_id != video_id:\n",
    "                    chunks.append((prev_video_id, start, end))\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                    prev_frame_number = None\n",
    "                    prev_video_id = video_id\n",
    "\n",
    "                frame_number = int(image_path.name.split('_')[1].replace('.png', ''))\n",
    "                if prev_frame_number is None:\n",
    "                    start = frame_number\n",
    "                else:\n",
    "                    if frame_number == prev_frame_number + 1:\n",
    "                        end = frame_number\n",
    "                    else:\n",
    "                        chunks.append((video_id, start, end))\n",
    "                        start = frame_number\n",
    "                prev_frame_number = frame_number\n",
    "            self.windows = []\n",
    "            for video_id, start, end in chunks:\n",
    "                for last_frame_index in range(start + self.window_size, end + 1):\n",
    "                    all_frames = []\n",
    "                    for frame_index in range(last_frame_index - self.window_size * self.downsample_factor, last_frame_index, self.downsample_factor):\n",
    "                        if frame_index < start or frame_index > end:\n",
    "                            continue\n",
    "                        all_frames.append(frame_index)\n",
    "                    if len(all_frames) == self.window_size:\n",
    "                        self.windows.append((video_id, all_frames))\n",
    "\n",
    "        if balance_data:\n",
    "            self.do_balance_data(task_type, temporal)\n",
    "\n",
    "    def do_balance_data(self, task_type, temporal):\n",
    "        print('Balancing data by oversampling under-represented classes...')\n",
    "        class_to_samples = defaultdict(list)\n",
    "        if not temporal:\n",
    "            for image_name in self.image_names:\n",
    "                if task_type == 'classification':\n",
    "                    label = self.classification_annotations[image_name]\n",
    "                elif task_type == 'segmentation':\n",
    "                    label = self.has_liver[image_name]\n",
    "                elif task_type == 'workflow' and not temporal:\n",
    "                    label = self.workflow_phase_annotations[image_name]\n",
    "                class_to_samples[label].append(image_name)\n",
    "            max_number = max([len(elem) for elem in class_to_samples.values()])\n",
    "            self.image_names = []\n",
    "            for key, value in class_to_samples.items():\n",
    "                if len(value) < max_number:\n",
    "                    self.image_names += random.choices(value, k=max_number)\n",
    "                else:\n",
    "                    self.image_names += value\n",
    "            random.shuffle(self.image_names)\n",
    "        else:\n",
    "            for video_id, window in self.windows:\n",
    "                label = self.workflow_phase_annotations[f'video{video_id}_{str(window[-1]).zfill(6)}']\n",
    "                class_to_samples[label].append((video_id, window))\n",
    "            max_number = max([len(elem) for elem in class_to_samples.values()])\n",
    "            self.windows = []\n",
    "            for key, value in class_to_samples.items():\n",
    "                if len(value) < max_number:\n",
    "                    self.windows += random.choices(value, k=max_number)\n",
    "                else:\n",
    "                    self.windows += value\n",
    "            random.shuffle(self.windows)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_type == 'workflow' and self.temporal:\n",
    "            return len(self.windows)\n",
    "        else:\n",
    "            return len(self.image_names)\n",
    "\n",
    "    def phase_label_to_number(self, phase_label):\n",
    "        if phase_label == 'Preparation':\n",
    "            return 0\n",
    "        elif phase_label == 'CalotTriangleDissection':\n",
    "            return 1\n",
    "        elif phase_label == 'GallbladderDissection':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError('Unknown phase label: {}'.format(phase_label))\n",
    "\n",
    "    def number_to_phase_label(self, phase_number):\n",
    "        if phase_number == 0:\n",
    "            return 'Preparation'\n",
    "        elif phase_number == 1:\n",
    "            return 'CalotTriangleDissection'\n",
    "        elif phase_number == 2:\n",
    "            return 'GallbladderDissection'\n",
    "        else:\n",
    "            raise ValueError('Unknown phase number: {}'.format(phase_number))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.task_type == 'workflow' and self.temporal:\n",
    "            video_id, window = self.windows[index]\n",
    "            image_paths = []\n",
    "            for frame_number in window:\n",
    "                image_paths.append(self.images_path / f'video{video_id}_{str(frame_number).zfill(6)}.png')\n",
    "\n",
    "            image_tensors = []\n",
    "            for image_path in image_paths:\n",
    "                image = Image.open(image_path)\n",
    "                if self.pil_transform is not None:\n",
    "                    image = self.pil_transform(image)\n",
    "                image_tensor = transforms.ToTensor()(image)\n",
    "                if self.tensor_transform is not None:\n",
    "                    image_tensor = self.tensor_transform(image_tensor)\n",
    "                image_tensors.append(image_tensor)\n",
    "\n",
    "            phase = self.phase_label_to_number(self.workflow_phase_annotations[image_paths[-1].name.replace('.png', '')])\n",
    "            image_tensors = torch.stack(image_tensors)\n",
    "            return image_tensors, phase\n",
    "        else:\n",
    "            image_name = self.image_names[index]\n",
    "            image_path = self.images_path / f'{image_name}.png'\n",
    "            seg_mask_path = self.seg_masks_path / f'{image_name}_liver_mask.png'\n",
    "            image = Image.open(image_path)\n",
    "            if self.pil_transform is not None:\n",
    "                image = self.pil_transform(image)\n",
    "            image_tensor = transforms.ToTensor()(image)\n",
    "            if self.tensor_transform is not None:\n",
    "                image_tensor = self.tensor_transform(image_tensor)\n",
    "            seg_mask = Image.open(seg_mask_path)\n",
    "            if self.pil_transform is not None:\n",
    "                seg_mask = self.pil_transform(seg_mask)\n",
    "            seg_mask_tensor = transforms.ToTensor()(seg_mask)[0].float()\n",
    "            if self.tensor_transform is not None:\n",
    "                seg_mask_tensor = self.tensor_transform(seg_mask_tensor)\n",
    "            intrument_exists = int(self.classification_annotations[image_name])\n",
    "            phase = self.phase_label_to_number(self.workflow_phase_annotations[image_name])\n",
    "\n",
    "            if self.task_type == 'classification':\n",
    "                return image_tensor, intrument_exists\n",
    "            elif self.task_type == 'segmentation':\n",
    "                return image_tensor, seg_mask_tensor\n",
    "            elif self.task_type == 'workflow' and not self.temporal:\n",
    "                return image_tensor, phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test = LiverEndoscopy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 854])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WkEZJPTvB1d"
   },
   "source": [
    "# Classification 1\n",
    "###### You need to define classification model with two CNN layers, ReLU and pooling operator afterwards and three Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RiLOyBiwu-8a",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # define the layers\n",
    "        # conv1: 3 filters, kernel_size = 5\n",
    "        # maxpooling: size = 2 \n",
    "        # conv1: 6 filters, kernel_size = 5\n",
    "        # fc1: output features = 120\n",
    "        # fc2: output features = 84\n",
    "        # fc3: output features = num_classes\n",
    "        #######################################\n",
    "        self.conv1 = nn.Conv2d(3, 3, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16854, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYmaCql8eOfN"
   },
   "source": [
    "# Classification 2\n",
    "###### You need to define classification model with pretrained ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ylL2AEpCich8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # define the backbone using ResNet18\n",
    "        #######################################\n",
    "        self.image_backbone = models.resnet18(pretrained = True)\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # disable the last layer, redefine it to match the number of classes\n",
    "        #######################################\n",
    "        self.image_backbone.fc = nn.Identity()\n",
    "        self.linear = nn.Linear(512, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_backbone(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.image_backbone.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JqDs9RRvbvi"
   },
   "source": [
    "# Classification 3\n",
    "###### Train both models with CrossEntropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "r1SPhJ6Env2M",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # use the model from Classification 1\n",
    "        # define the loss\n",
    "        #######################################\n",
    "        self.model = ClassificationModel()\n",
    "        # define the loss\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_preds = []\n",
    "        self.train_gts = []\n",
    "        self.val_preds = []\n",
    "        self.val_gts = []\n",
    "        self.test_preds = []\n",
    "        self.test_gts = []\n",
    "        self.reset_metrics()\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.update_metrics(y, y_hat, split='train')\n",
    "        self.train_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "        x, y = batch\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.update_metrics(y, y_hat, split='val')\n",
    "        self.val_loss.append(loss.item())\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        x, y = batch\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.update_metrics(y, y_hat, split='test')\n",
    "        self.test_loss.append(loss.item())\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # Define Adam optimizer with different learning rates in range (1e-2, 1e-4) \n",
    "        #######################################\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def reset_metrics(self, split=None):\n",
    "        if split == 'train':\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "        elif split == 'val':\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "        elif split == 'test':\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "        else:\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "\n",
    "    def update_metrics(self, gt, pred, split='train'):\n",
    "        if split == 'train':\n",
    "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'val':\n",
    "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'test':\n",
    "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='train')\n",
    "        self.reset_metrics(split='train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='val')\n",
    "        self.reset_metrics(split='val')\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='test')\n",
    "        self.reset_metrics(split='test')\n",
    "\n",
    "    def evaluate_predictions(self, split):\n",
    "        if split == 'train':\n",
    "            preds = self.train_preds\n",
    "            gts = self.train_gts\n",
    "        elif split == 'val':\n",
    "            preds = self.val_preds\n",
    "            gts = self.val_gts\n",
    "        elif split == 'test':\n",
    "            preds = self.test_preds\n",
    "            gts = self.test_gts\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        cls_report = classification_report(gts, preds)\n",
    "        print(split)\n",
    "        print(cls_report)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XJCw4EMWMUU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # use the Resnet model from Classification 2\n",
    "        # define the loss\n",
    "        #######################################\n",
    "        self.model = ResNetModel()\n",
    "        # define the loss\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_preds = []\n",
    "        self.train_gts = []\n",
    "        self.val_preds = []\n",
    "        self.val_gts = []\n",
    "        self.test_preds = []\n",
    "        self.test_gts = []\n",
    "        self.reset_metrics()\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.update_metrics(y, y_hat, split='train')\n",
    "        self.train_loss.append(loss.item())\n",
    "        return {'loss': loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "        x, y = batch\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.update_metrics(y, y_hat, split='val')\n",
    "        self.val_loss.append(loss.item())\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        x, y = batch\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # run the model on input image to get the prediction y_hat\n",
    "        # specify the loss inputs\n",
    "        #######################################\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.update_metrics(y, y_hat, split='test')\n",
    "        self.test_loss.append(loss.item())\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #######################################\n",
    "        # To be filled\n",
    "        #\n",
    "        # Define Adam optimizer with different learning rates in range (1e-2, 1e-4) \n",
    "        #######################################\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def reset_metrics(self, split=None):\n",
    "        if split == 'train':\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "        elif split == 'val':\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "        elif split == 'test':\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "        else:\n",
    "            self.train_preds = []\n",
    "            self.train_gts = []\n",
    "            self.val_preds = []\n",
    "            self.val_gts = []\n",
    "            self.test_preds = []\n",
    "            self.test_gts = []\n",
    "\n",
    "    def update_metrics(self, gt, pred, split='train'):\n",
    "        if split == 'train':\n",
    "            self.train_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.train_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'val':\n",
    "            self.val_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.val_gts.extend(gt.detach().cpu().numpy())\n",
    "        elif split == 'test':\n",
    "            self.test_preds.extend(pred.detach().cpu().numpy().argmax(1))\n",
    "            self.test_gts.extend(gt.detach().cpu().numpy())\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='train')\n",
    "        self.reset_metrics(split='train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='val')\n",
    "        self.reset_metrics(split='val')\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.evaluate_predictions(split='test')\n",
    "        self.reset_metrics(split='test')\n",
    "\n",
    "    def evaluate_predictions(self, split):\n",
    "        if split == 'train':\n",
    "            preds = self.train_preds\n",
    "            gts = self.train_gts\n",
    "        elif split == 'val':\n",
    "            preds = self.val_preds\n",
    "            gts = self.val_gts\n",
    "        elif split == 'test':\n",
    "            preds = self.test_preds\n",
    "            gts = self.test_gts\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        cls_report = classification_report(gts, preds)\n",
    "        print(split)\n",
    "        print(cls_report)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpGvnV9qlDlI"
   },
   "source": [
    "# Dataloaders \n",
    "###### Define the validation and test datasets simular to training dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70FwxKw8PoJp",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pil_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "train_dataset = LiverEndoscopy(split='train', balance_data=True, task_type='classification', temporal=False, pil_transform=pil_transform)\n",
    "#######################################\n",
    "# To be filled\n",
    "# Define the validation and test datasets \n",
    "# simular to training dataset\n",
    "#######################################\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "#######################################\n",
    "# To be filled\n",
    "# Define loaders with batch_size = 64 and \n",
    "# shuffle=True for train loader and \n",
    "# shuffle=False for the rest, num_workers=1\n",
    "#######################################\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "#######################################\n",
    "# To be filled\n",
    "# Define the model\n",
    "#######################################\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgzLIhvAPzRZ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=3, num_sanity_val_steps=0)\n",
    "                     \n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tO-MfgRP79q",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification CAMP 2 Exercise.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
